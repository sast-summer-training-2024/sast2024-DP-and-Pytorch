{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy基础\n",
    "数据处理库。核心是多维数组NDarray\n",
    "底层使用C实现，效率高。\n",
    "\n",
    "## NDArray\n",
    "numpy中最重要的类。多维数据包装器\n",
    "\n",
    "多维数组的创建、修改以及多维数组的属性\n",
    "\n",
    "### 创建\n",
    "从数组创建或使用内置函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "arr2 = np.zeros([2, 2, 3], dtype = np.float32)\n",
    "arr3 = np.ones([2, 2])\n",
    "print(arr1, type(arr1))\n",
    "print(arr2)\n",
    "print(arr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改\n",
    "主要包括维度变换和拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.zeros([1, 1, 4],  dtype=np.int32)\n",
    "arr2 = np.ones([5, 1, 4], dtype=np.int32)\n",
    "arr = np.concatenate([arr1, arr2], axis=0)\n",
    "print('concatenate:', arr)\n",
    "\n",
    "print('expand1:', np.expand_dims(arr1, 3))\n",
    "print('expand2', arr2[ :, :, np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数组属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 1, 4, 5, 1, 4], [1, 9, 1, 9, 8, 10]])\n",
    "print('size:', arr.size)\n",
    "print('shape:', arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切片和筛选\n",
    "切片和python list一致\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 1, 4, 5, 1], \n",
    "                [4, 1, 9, 1, 9],\n",
    "                [8, 1, 0, 1, 1], \n",
    "                [4, 5, 1, 4, 1], \n",
    "                [9, 1, 9, 8, 10]])\n",
    "\n",
    "print('single choose')\n",
    "print(arr[1])\n",
    "print(arr[1, 0])\n",
    "print(arr[[1, 0], [2, 3]])\n",
    "\n",
    "print('slice')\n",
    "print(arr[:2, :3])\n",
    "\n",
    "print('filter')\n",
    "print(arr>5)\n",
    "print(arr[arr>7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.where\n",
    "按条件选择、替换数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = arr>5\n",
    "print(condition)\n",
    "print(np.where(condition, -1, arr))\n",
    "print(np.where(condition, -1, 2))\n",
    "rra = -arr\n",
    "print(np.where(condition, arr, rra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据保存和加载\n",
    "```python\n",
    "numpy.save(file, arr, allow_pickle=True, fix_imports=True)\n",
    "```\n",
    "- file：要保存的文件，扩展名为 .npy，如果文件路径末尾没有扩展名 .npy，该扩展名会被自动加上。\n",
    "- arr: 要保存的数组\n",
    "\n",
    "```python\n",
    "numpy.savez(file, *args, **kwds)\n",
    "```\n",
    "- file：要保存的文件，扩展名为 .npz，如果文件路径末尾没有扩展名 .npz，该扩展名会被自动加上。\n",
    "- args: 要保存的数组，可以使用关键字参数为数组起一个名字，非关键字参数传递的数组会自动起名为 arr_0, arr_1, …　。\n",
    "- kwds: 要保存的数组使用关键字名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "a = np.array([1, 1, 4, 5, 1, 4])\n",
    "b = np.array([[1, 9, 1], [9, 8, 10]])\n",
    "\n",
    "np.save(\"result/a\", a)\n",
    "np.savez(\"result/ab\", a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载\n",
    "a = np.load('result/a.npy')\n",
    "ab = np.load('result/ab.npz')\n",
    "\n",
    "print(a)\n",
    "print(ab)\n",
    "print(ab['a'])\n",
    "print(ab['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast\n",
    "一种自动复制填充机制，使得原本形状不同的两个array能够进行原本只有两相同形状array才能进行的操作。\n",
    "\n",
    "广播的规则如下：\n",
    "\n",
    "1. 从后向前，如果两数组对应维度上轴的长度相同或其中一个的轴长度为1，广播兼容，可在轴长度为1的轴上进行广播机制处理。\n",
    "2. 如果两个数组的维度不同导致某个数组的前方没有维度，那么给低维度的数组前扩展提升一维，扩展维的轴长度为1,然后在扩展出的维上进行广播机制处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a: (3, 5)\n",
    "b: (5)\n",
    "\n",
    "1. extend b into (1, 5)\n",
    "2. copy b in dim 0 to (3, 5)\n",
    "'''\n",
    "a = np.arange(1, 16).reshape([3, 5])\n",
    "print(a)\n",
    "b = np.array([2, 3, 4, 5, 6])\n",
    "print(b)\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2: ()\n",
    "a: (3, 5)\n",
    "\n",
    "1. extend 2 into (1)\n",
    "2. copy 2 in dim 0 to (5)\n",
    "3. extend 2 into (1, 5)\n",
    "4. copy 2 in dim 0 to (3, 5)\n",
    "\"\"\"\n",
    "\n",
    "a = np.arange(1, 16).reshape([3, 5])\n",
    "print(2 * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a: (1, 5)\n",
    "b: (3, 4, 1)\n",
    "\n",
    "1. copy b in dim 2 to (3, 4, 5)\n",
    "2. copy a in dim 0 to (4, 5)\n",
    "3. extend a into (1, 4, 5)\n",
    "4. copy a in dim 0 to (3, 4, 5)\n",
    "'''\n",
    "\n",
    "a = np.arange(1, 5)\n",
    "b = np.arange(0, 12).reshape((3, 4, 1))\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 矩阵和向量积\n",
    "a = np.random.randint(5, size=(3, 3))\n",
    "b = np.random.randint(5, size = (3, 3))\n",
    "\n",
    "print(f'a:{a}\\nb:{b}')\n",
    "\n",
    "# dot根据不同情况选择，1维数组为内积，2维数组为矩阵乘，其他情况请阅读文档\n",
    "print(f'dot:\\n {np.dot(a, b)}')\n",
    "# vdot计算逐元素乘积并求和\n",
    "print(f\"vdot:\\n {np.vdot(a, b)}\")\n",
    "# inner 计算最后一维内积\n",
    "print(f\"inner:\\n {np.inner(a, b)}\")\n",
    "# 展开后算ab^T\n",
    "print(f\"outer:\\n {np.outer(a, b)}\")\n",
    "# matmul计算矩阵乘\n",
    "print(f\"outer:\\n {a@b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstein Notation\n",
    "$a_i b_i = \\sum_{i} a_i b_i$\n",
    "\n",
    "$a_{ij}b_{jk} = \\sum_{j}a_{ij}b_{jk}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = np.arange(0, 9).reshape(3, 3)\n",
    "print(a)\n",
    "print(np.einsum(\"ij->ji\", a))\n",
    "\n",
    "\n",
    "A = torch.tensor(np.random.randint(5, size=(2, 2, 2, 5))).reshape(2, 2, 1, 2, 5)\n",
    "B = torch.tensor(np.random.randint(5, size=(2, 2, 2, 5))).reshape(2, 1, 2, 2, 5)\n",
    "\"\"\"\n",
    "想将两者的第2个维度分别reshape为2x1和1x2的两个向量，然后计算外积，得到一个2x2的kernel\n",
    "\"\"\"\n",
    "print(A.shape, B.shape)\n",
    "res = torch.einsum(\n",
    "    \"ijk...,iko...->ijo...\", [A, B]\n",
    ")\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy使用原则\n",
    "尽量向量化所有操作，让numpy可以自动并行\n",
    "\n",
    "1. 减少遍历操作\n",
    "2. 尽量使用内置方法和函数\n",
    "3. 善用广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s_time = time.time()\n",
    "a = np.array([i for i in range(1919810)])\n",
    "print(time.time()-s_time)\n",
    "\n",
    "s_time = time.time()\n",
    "a = np.linspace(0, 1919810, 1919811)\n",
    "print(time.time() - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random(1919810)\n",
    "b = np.random.random(1919810)\n",
    "\n",
    "s_time = time.time()\n",
    "print(a@b)\n",
    "print(time.time() - s_time)\n",
    "\n",
    "s_time = time.time()\n",
    "res = 0\n",
    "for i in range(len(a)):\n",
    "    res += a[i]*b[i]\n",
    "print(res)\n",
    "print(time.time() - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.random((1145, 14))\n",
    "\n",
    "\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "dists = []\n",
    "for p1 in samples:\n",
    "    for p2 in samples:\n",
    "        dists.append(np.linalg.norm(p1-p2))\n",
    "print(np.mean(dists))\n",
    "\n",
    "print(time.time() - s_time)\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "diff = samples[:, np.newaxis, :] - samples[np.newaxis, :, :]\n",
    "dist = np.linalg.norm(diff, axis = -1)\n",
    "print(np.mean(dist))\n",
    "\n",
    "print(time.time() - s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib绘图\n",
    "以曲线为例\n",
    "## 基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-np.pi, np.pi, 256, endpoint=True)\n",
    "C, S = np.cos(X), np.sin(X)\n",
    "\n",
    "plt.plot(X, C)\n",
    "plt.plot(X, S)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "\n",
    "X = np.linspace(-np.pi, np.pi, 30, endpoint=True)\n",
    "C, S, T, A = np.cos(X), np.sin(X), np.arctan(X), np.arccos(X)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "# 绘制余弦曲线，使用蓝色的、连续的、宽度为 1 （像素）的线条\n",
    "plt.plot(X, C, color=\"blue\", linewidth=1.0, linestyle=\"-\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "# 绘制正弦曲线，使用绿色的、连续的、宽度为 1 （像素）的线条\n",
    "plt.plot(X, S, color=\"green\", linewidth=4.0, linestyle=\"-.\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X, T, color='red', alpha=0.5)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(X, T, color=\"yellow\", alpha=0.5)\n",
    "\n",
    "# 在屏幕上显示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-np.pi, np.pi, 256, endpoint=True)\n",
    "C, S = np.cos(X), np.sin(X)\n",
    "\n",
    "plt.plot(X, C, label = 'sin')\n",
    "plt.plot(X, S, label='cos')\n",
    "\n",
    "# 修改标签\n",
    "plt.xticks(\n",
    "    [-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi],\n",
    "    [r\"$-\\pi$\", r\"$-\\pi/2$\", r\"$0$\", r\"$+\\pi/2$\", r\"$+\\pi$\"],\n",
    ")\n",
    "\n",
    "plt.yticks([-1, 0, +1], [r\"$-1$\", r\"$0$\", r\"$+1$\"])\n",
    "\n",
    "# 修改坐标轴位置\n",
    "ax = plt.gca()\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "ax.xaxis.set_ticks_position(\"bottom\")\n",
    "ax.spines[\"bottom\"].set_position((\"data\", 0))\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "ax.spines[\"left\"].set_position((\"data\", 0))\n",
    "\n",
    "# 显示图例\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# 标注\n",
    "t = 2 * np.pi / 3\n",
    "plt.scatter(\n",
    "    [\n",
    "        t,\n",
    "    ],\n",
    "    [\n",
    "        np.sin(t),\n",
    "    ],\n",
    "    s=100,\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.annotate(\n",
    "    r\"$\\sin(\\frac{2\\pi}{3})=\\frac{\\sqrt{3}}{2}$\",\n",
    "    xy=(t, np.sin(t)),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(+10, +30),\n",
    "    textcoords=\"offset points\",\n",
    "    arrowprops= dict(arrowstyle = \"->\"),\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Scipy\n",
    "> an open-source software for mathematics, science, and engineering.\n",
    "## Document\n",
    "https://docs.scipy.org/doc/scipy/reference/index.html\n",
    "## constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.constants\n",
    "print(dir(scipy.constants))\n",
    "\n",
    "print(scipy.constants.Avogadro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "\n",
    "def eqn(x):\n",
    "    return x + cos(x)\n",
    "\n",
    "myroot = scipy.optimize.root_scalar(eqn, method='newton', x0=0.1)\n",
    "\n",
    "print(myroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn(x):\n",
    "    return (x[0]-3)**2 + (x[1]-1)**2\n",
    "\n",
    "scipy.optimize.minimize(eqn, x0=[0, 0], method='CG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import connected_components, dijkstra, floyd_warshall\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "arr = np.array([[0, 1, 1], \n",
    "                [1, 0, 1111], \n",
    "                [2, 1111, 0]])\n",
    "\n",
    "newarr = csr_matrix(arr)\n",
    "\n",
    "print(connected_components(newarr))\n",
    "print(dijkstra(newarr, return_predecessors=True, indices=2))\n",
    "print(floyd_warshall(newarr, return_predecessors=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistics\n",
    "### 随机变量相关\n",
    "\n",
    "连续随机变量对象有下列方法：\n",
    "- rvs：采样\n",
    "- pdf：密度函数\n",
    "- cdf：累积分布函数\n",
    "- sf：残存函数\n",
    "- ppf：累积分布的逆\n",
    "- isf：残存函数的逆\n",
    "- fit：最大似然估计求参数\n",
    "\n",
    "离散分布的简单方法大多数与连续分布很类似，但是pdf被更换为密度函数pmf。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从常用分布中采样\n",
    "from scipy import stats\n",
    "rv = stats.norm(\n",
    "    loc=0,\n",
    "    scale=1\n",
    ")\n",
    "print(rv.rvs(size=10))\n",
    "print(rv.pdf(1))\n",
    "\n",
    "data = stats.norm.rvs(loc=114, scale=514, size=1000000)\n",
    "print(stats.norm.fit(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 假设检验\n",
    "常用的如下：\n",
    "- 单样本均值检验\n",
    "- 两独立样本均值检验\n",
    "    - 方差相等\n",
    "    - 方差不相等\n",
    "    - 检验方差\n",
    "- 配对样本均值检"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_1samp, levene, ttest_rel\n",
    "import random\n",
    "\n",
    "v1 = np.random.normal(size=1000)\n",
    "v2 = np.random.normal(loc=1, size=1000)\n",
    "\n",
    "# 单总体方差\n",
    "res = ttest_1samp(v1, 0)\n",
    "print(res)\n",
    "\n",
    "# 两独立总体\n",
    "res = ttest_ind(v1, v2, equal_var=True)\n",
    "print(res)\n",
    "\n",
    "# 两总体方差\n",
    "res = levene(v1, v2)\n",
    "print(res)\n",
    "\n",
    "v_pair = [i + random.uniform(-1, 1) for i in v1]\n",
    "# 配对样本均值\n",
    "res= ttest_rel(v1, v_pair)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "- 拥有类似于NumPy的张量计算系统，能在GPU等硬件上加速\n",
    "- 自动微分系统的计算图在运行中构建（即动态图系统，区别于tensorflow先定义再建图的静态图系统）\n",
    "\n",
    "动态图带来的好处在具有复杂控制条件的网络中尤其明显。例如，pytorch可以直接进行如下实现：\n",
    "```python\n",
    "def forward(self, x)\n",
    "    for i in range(5):\n",
    "        cond = random.randint(1, 5)\n",
    "        if cond == 1:\n",
    "            x = self.fc1(x)\n",
    "        elif cond == 2:\n",
    "            x = self.fc2(2)\n",
    "        #.....\n",
    "    return x\n",
    "```\n",
    "而使用tensorflow则需要使用tf.cond()进行多层嵌套\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch模型训练八股文\n",
    "```python\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, *args):\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "    def __len__(self)\n",
    "\n",
    "dataset = MyDataset(*args)\n",
    "dataloader = DataLoader(dataset, *args)\n",
    "\n",
    "class Model(nn.Modules):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return output\n",
    "\n",
    "model = Model(*args)\n",
    "\n",
    "lossfn = Loss()\n",
    "optimizer = Optimizer(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch_num in range(EPOCH):\n",
    "    for (data, label) in dataloader:\n",
    "        output = model(data)\n",
    "        loss = lossfn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "torch.save(model, MODEL_SAVE_PATH)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch的模型定义\n",
    "pytorch中所有神经网络模型都由 torch.nn.Module 派生得到。在实现自己的模型时，绝大多数情况下只需要重写__init__, forward两个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        # 简写：super().__init__()\n",
    "        self.layer = torch.nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return(self.layer(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytoch梯度计算与模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = TestModel()\n",
    "data = torch.rand((1, 5))\n",
    "y = test_model(data)\n",
    "\n",
    "optimizer = torch.optim.SGD(test_model.parameters(), lr=1)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "print(\"-------------------\\nbefore loss backward:\\n\")\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "\n",
    "loss = loss_fn(torch.randn((1, 2)), y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"-------------------\\nafter loss backward:\\n\")\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "print(test_model.layer._parameters[\"weight\"].grad.shape)\n",
    "\n",
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"-------------------\\nafter optimize:\")\n",
    "\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "print(test_model.layer._parameters[\"weight\"].grad.shape)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "print(\"-------------------\\nafter zerograd:\")\n",
    "\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = loss_fn(torch.randn((1, 2)), y)\n",
    "    print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer/tokenizer.json\")\n",
    "tokenizer.enable_padding(length=256)\n",
    "\n",
    "def deal_segment(inputlist, savelist):\n",
    "    for item in inputlist:\n",
    "        tmp_dict = json.loads(item)\n",
    "        savelist.append(\n",
    "            (\n",
    "                torch.tensor(tokenizer.encode(tmp_dict[\"content\"]).ids[:256]),\n",
    "                torch.tensor(\n",
    "                    [1 - tmp_dict[\"label\"], tmp_dict[\"label\"]], dtype=torch.float32\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, file: str):\n",
    "        self.data = []\n",
    "        with open(file, \"r\", encoding='utf-8') as fin:\n",
    "            inputlist = list(fin)\n",
    "            tlist = [threading.Thread(target=deal_segment, args = (inputlist[1000*i:1000*(i+1)], self.data,)) for i in range(math.ceil(len(inputlist)/1000))]\n",
    "            for t in tqdm(tlist):\n",
    "                t.start()\n",
    "            for t in tlist:\n",
    "                t.join()\n",
    "        fin.close()\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_set = MyDataSet(file=\"dataset/train.jsonl\")\n",
    "test_set = MyDataSet(file=\"dataset/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(data)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模型定义\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=50000, embedding_dim=64)\n",
    "        self.layer1 = nn.Linear(256*64, 64*128)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64*128, 16*16)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.out = nn.Linear(16*16, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        hidden = self.emb(data).view(-1, 64*256)\n",
    "        return self.out(self.ac2(self.layer2(self.ac1(self.layer1(hidden)))))\n",
    "    \n",
    "    \n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "# 损失函数和优化器定义\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "# 训练\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"summer_guide\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"architecture\": \"MLP\",\n",
    "        \"dataset\": \"amazon-plarity\",\n",
    "        \"epochs\": 3,\n",
    "    },\n",
    ")\n",
    "# model.cuda()\n",
    "for i in range(wandb.config['epochs']):\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        pred = model(X.cuda())\n",
    "        loss = loss_fn(pred, y.cuda())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"loss\": loss,\n",
    "                    \"acc\": np.mean((torch.argmax(pred.cpu(), 1) == torch.argmax(y.cpu(), 1)).numpy()),\n",
    "                }\n",
    "            )\n",
    "    torch.save(model.state_dict(), \"result/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"result/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理\n",
    "# model = MyModel()\n",
    "# model.load_state_dict(torch.load('result/model.pt'))\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "model = torch.load(\n",
    "    \"result/model.pt\", map_location=torch.device(\"cpu\"), weights_only=False\n",
    ")\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer/tokenizer.json\")\n",
    "tokenizer.enable_padding(length=256)\n",
    "# sentence = \"I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\"\n",
    "sentence = \"My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \\\"Who was that singing ?\\\"\"\n",
    "ids = torch.tensor(tokenizer.encode(sentence).ids)\n",
    "print(\"origin sentence:\", tokenizer.decode(ids.tolist()), '\\n')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = torch.nn.functional.softmax(model(ids), dim=-1)\n",
    "    print(\"probability:\", out, '\\n')\n",
    "    print(\"class:\", torch.argmax(out).item(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业\n",
    "编写脚本，实现RNN文本情感分类任务，要求如下：\n",
    "- 使用GRU或LSTM块作为RNN块（推荐手写，但也可以使用pytorch内置的类，如果想使用请阅读pytorch文档）\n",
    "- RNN能够实现不定长输入，即根据输入长度判断RNN的循环次数\n",
    "- 分类正确率超过80%\n",
    "- 能够使用plt或wandb绘制训练过程的loss、train accuracy和test accuracy曲线\n",
    "- 能够保存训练后的神经网络参数，以及加载并使用该参数进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
