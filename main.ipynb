{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy基础\n",
    "数据处理库。核心是多维数组NDarray\n",
    "底层使用C实现，效率高。\n",
    "\n",
    "## NDArray\n",
    "numpy中最重要的类。多维数据包装器\n",
    "\n",
    "多维数组的创建、修改以及多维数组的属性\n",
    "\n",
    "### 创建\n",
    "从数组创建或使用内置函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "arr2 = np.zeros([2, 2, 3], dtype = np.float32)\n",
    "arr3 = np.ones([2, 2])\n",
    "print(arr1, type(arr1))\n",
    "print(arr2)\n",
    "print(arr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改\n",
    "主要包括维度变换和拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.zeros([1, 1, 4],  dtype=np.int32)\n",
    "arr2 = np.ones([5, 1, 4], dtype=np.int32)\n",
    "arr = np.concatenate([arr1, arr2], axis=0)\n",
    "print('concatenate:', arr)\n",
    "\n",
    "print('expand1:', np.expand_dims(arr1, 3))\n",
    "print('expand2', arr2[ :, :, np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数组属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 1, 4, 5, 1, 4], [1, 9, 1, 9, 8, 10]])\n",
    "print('size:', arr.size)\n",
    "print('shape:', arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切片和筛选\n",
    "切片和python list一致\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 1, 4, 5, 1], \n",
    "                [4, 1, 9, 1, 9],\n",
    "                [8, 1, 0, 1, 1], \n",
    "                [4, 5, 1, 4, 1], \n",
    "                [9, 1, 9, 8, 10]])\n",
    "\n",
    "print('single choose')\n",
    "print(arr[1])\n",
    "print(arr[1, 0])\n",
    "print(arr[[1, 0], [2, 3]])\n",
    "\n",
    "print('slice')\n",
    "print(arr[:2, :3])\n",
    "\n",
    "print('filter')\n",
    "print(arr>5)\n",
    "print(arr[arr>7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.where\n",
    "按条件选择、替换数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = arr>5\n",
    "print(np.where(condition, -1, arr))\n",
    "print(np.where(condition, -1, 2))\n",
    "rra = -arr\n",
    "print(np.where(condition, arr, rra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载txt格式的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return(self.layer(data))\n",
    "\n",
    "\n",
    "test_model = TestModel()\n",
    "data = torch.rand((1, 5))\n",
    "y = test_model(data)\n",
    "\n",
    "optimizer = torch.optim.SGD(test_model.parameters(), lr = 1)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "print(\"-------------------\\nbefore loss backward:\\n\")\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "\n",
    "loss = loss_fn(torch.randn((1,2)), y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"-------------------\\nafter loss backward:\\n\")\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "print(test_model.layer._parameters[\"weight\"].grad.shape)\n",
    "\n",
    "print(loss.grad_fn.next_functions)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"-------------------\\nafter optimize:\\n\")\n",
    "\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "print(test_model.layer._parameters[\"weight\"].grad.shape)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "print(\"-------------------\\nafter zerograd:\\n\")\n",
    "\n",
    "print(test_model.layer._parameters[\"weight\"])\n",
    "print(test_model.layer._parameters[\"weight\"].grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = loss_fn(torch.randn((1, 2)), y)\n",
    "    print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer/tokenizer.json\")\n",
    "tokenizer.enable_padding(length=256)\n",
    "# print(tokenizer.encode(\"Hace a nice Day!\").ids)\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, file: str):\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        with open(file, \"r\", encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                tmp_dict = json.loads(line)\n",
    "                self.data.append(torch.tensor(tokenizer.encode(tmp_dict[\"content\"]).ids[:256]))\n",
    "                self.label.append(torch.tensor([1-tmp_dict[\"label\"], tmp_dict['label']], dtype=torch.float32))\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_set = MyDataSet(file=\"dataset/train.jsonl\")\n",
    "test_set = MyDataSet(file=\"dataset/test.jsonl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(data)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=50000, embedding_dim=64)\n",
    "        self.layer1 = nn.Linear(256*64, 64*128)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64*128, 16*16)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.out = nn.Linear(16*16, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        hidden = self.emb(data).view(-1, 64*256)\n",
    "        return self.out(self.ac2(self.layer2(self.ac1(self.layer1(hidden)))))\n",
    "    \n",
    "    \n",
    "model = MyModel()\n",
    "# print(model(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"summer_guide\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"architecture\": \"MLP\",\n",
    "        \"dataset\": \"amazon-plarity\",\n",
    "        \"epochs\": 1,\n",
    "    },\n",
    ")\n",
    "model.cuda()\n",
    "for i in range(20):\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        pred = model(X.cuda())\n",
    "        loss = loss_fn(pred, y.cuda())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"loss\": loss,\n",
    "                    \"acc\": np.mean((torch.argmax(pred.cpu(), 1) == torch.argmax(y.cpu(), 1)).numpy()),\n",
    "                }\n",
    "            )\n",
    "    torch.save(model, \"result/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"result/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
